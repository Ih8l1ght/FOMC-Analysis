{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idioms and Metaphores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries and Download NLTK Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = './'  # Modify this if your project folder is located elsewhere\n",
    "idioms_file_path = os.path.join(project_folder, 'idioms.txt')\n",
    "files_text_folder = os.path.join(project_folder, 'Files_Text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Read File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Vectorize Idioms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short List of Idioms Vectorized:\n",
      "  (0, 406)\t0.007876702166955815\n",
      "  (0, 137)\t0.005251134777970542\n",
      "  (0, 678)\t0.002625567388985271\n",
      "  (0, 654)\t0.007876702166955815\n"
     ]
    }
   ],
   "source": [
    "idioms_text = read_file(idioms_file_path)\n",
    "if not idioms_text:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "idioms_vector = vectorizer.fit_transform([idioms_text])\n",
    "\n",
    "N = 20\n",
    "\n",
    "print(\"Short List of Idioms Vectorized:\")\n",
    "print(str(idioms_vector)[:127])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text Files in `Files_Text` Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "files_text_files = glob.glob(os.path.join(files_text_folder, '*.txt'))\n",
    "tokenized_texts = {}\n",
    "for file_path in files_text_files:\n",
    "    file_content = read_file(file_path)\n",
    "    if file_content:\n",
    "        tokenized_texts[file_path] = tokenize_text(file_content)\n",
    "        print(f\"Tokenized {file_path} with {len(tokenized_texts[file_path])} tokens.\")\n",
    "    else:\n",
    "        print(f\"Skipped empty file: {file_path}\")\n",
    "\n",
    "# Output tokenized results\n",
    "for file_path, tokens in tokenized_texts.items():\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    print(f\"Tokens: {tokens[:10]}...\")  # Printing only the first 10 tokens for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Idioms and Extract from Text Files in `Text_Files` Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 717 idioms.\n",
      "Found 83 files in ./Text_Files.\n"
     ]
    }
   ],
   "source": [
    "# Define new folder path\n",
    "text_files_folder = os.path.join(project_folder, 'Text_Files')\n",
    "\n",
    "idioms_text = read_file(idioms_file_path)\n",
    "idioms = [line.strip() for line in idioms_text.splitlines() if line.strip()]\n",
    "if not idioms:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "print(f\"Extracted {len(idioms)} idioms.\")\n",
    "\n",
    "# Tokenize Text Files\n",
    "text_files_paths = glob.glob(os.path.join(text_files_folder, '*.txt'))\n",
    "print(f\"Found {len(text_files_paths)} files in {text_files_folder}.\")\n",
    "file_contents = [read_file(file_path) for file_path in text_files_paths]\n",
    "\n",
    "# Filter out empty texts\n",
    "file_contents = [text for text in file_contents if text]\n",
    "text_files_paths = [file_path for file_path, text in zip(text_files_paths, file_contents) if text]\n",
    "\n",
    "if not text_files_paths:\n",
    "    raise ValueError(\"No valid content in Text_Files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Exact Matches and Calculate Similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results = {}\n",
    "for file_path, content in zip(text_files_paths, file_contents):\n",
    "    tokens = tokenize_text(content)\n",
    "    matching_idioms = [idiom for idiom in idioms if idiom.lower() in content.lower()]\n",
    "    if matching_idioms:\n",
    "        results[file_path] = matching_idioms\n",
    "    else:\n",
    "        results[file_path] = \"No exact idiom matches.\"\n",
    "'''\n",
    "# Output exact matches\n",
    "print(\"\\nExact Matches:\")\n",
    "for file_path, matches in results.items():\n",
    "    #print(f\"\\nFile: {file_path}\")\n",
    "    if isinstance(matches, list):\n",
    "        print(f\"  Matching Idioms: {matches} in {file_path}\")\n",
    "    else:\n",
    "        print(f\"  {matches}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and Similarity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', lowercase=True, ngram_range=(1, 2))\n",
    "all_texts = idioms + file_contents\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Separate idioms vectors and text file vectors\n",
    "idioms_vectors = tfidf_matrix[:len(idioms)]\n",
    "text_files_vectors = tfidf_matrix[len(idioms):]\n",
    "\n",
    "# similarities\n",
    "similarities = cosine_similarity(text_files_vectors, idioms_vectors)\n",
    "\n",
    "# Output similarity results\n",
    "similarity_threshold = 0.1  # it's already low, just like my will to live\n",
    "print(\"\\nSimilarity Results:\")\n",
    "for idx, file_path in enumerate(text_files_paths):\n",
    "    similar_idioms = [idioms[j] for j in range(len(idioms)) if similarities[idx][j] >= similarity_threshold]\n",
    "    if similar_idioms:\n",
    "        print(f\"\\nFile: {file_path}\")\n",
    "        print(f\"  Similar Idioms (threshold {similarity_threshold}): {similar_idioms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# From Mixed Nuts to Metaphors: Decoding the Nuts and Bolts of Figurative Speech with SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to analyze metaphors in a given text\n",
    "def analyze_metaphors(text):\n",
    "    doc = nlp(text)\n",
    "    metaphor_count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"metaphor\":\n",
    "            metaphor_count += 1\n",
    "    return metaphor_count\n",
    "\n",
    "# Function to read a text file and return its content as a striiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiing\n",
    "def read_text_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Get the path to the Text_Files folder\n",
    "text_folder_path = \"Text_Files\"\n",
    "\n",
    "# Get all filenames within the folder\n",
    "filenames = os.listdir(text_folder_path)\n",
    "\n",
    "list_as_empty_as_my_wallet = []\n",
    "\n",
    "# Loopy loop\n",
    "for filename in filenames:\n",
    "    metaphor_n = analyze_metaphors(text)\n",
    "    list_as_empty_as_my_wallet.append(metaphor_n)\n",
    "print(list_as_empty_as_my_wallet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

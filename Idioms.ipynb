{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioms Vectorized:\n",
      "  (0, 406)\t0.007876702166955815\n",
      "  (0, 137)\t0.005251134777970542\n",
      "  (0, 678)\t0.002625567388985271\n",
      "  (0, 654)\t0.007876702166955815\n",
      "  (0, 604)\t0.005251134777970542\n",
      "  (0, 748)\t0.002625567388985271\n",
      "  (0, 133)\t0.002625567388985271\n",
      "  (0, 207)\t0.002625567388985271\n",
      "  (0, 614)\t0.002625567388985271\n",
      "  (0, 198)\t0.005251134777970542\n",
      "  (0, 579)\t0.007876702166955815\n",
      "  (0, 740)\t0.002625567388985271\n",
      "  (0, 324)\t0.002625567388985271\n",
      "  (0, 68)\t0.01575340433391163\n",
      "  (0, 112)\t0.002625567388985271\n",
      "  (0, 215)\t0.007876702166955815\n",
      "  (0, 812)\t0.005251134777970542\n",
      "  (0, 235)\t0.013127836944926356\n",
      "  (0, 204)\t0.002625567388985271\n",
      "  (0, 25)\t0.010502269555941085\n",
      "  (0, 438)\t0.013127836944926356\n",
      "  (0, 478)\t0.005251134777970542\n",
      "  (0, 103)\t0.002625567388985271\n",
      "  (0, 485)\t0.002625567388985271\n",
      "  (0, 161)\t0.007876702166955815\n",
      "  :\t:\n",
      "  (0, 175)\t0.002625567388985271\n",
      "  (0, 582)\t0.002625567388985271\n",
      "  (0, 620)\t0.002625567388985271\n",
      "  (0, 40)\t0.002625567388985271\n",
      "  (0, 650)\t0.002625567388985271\n",
      "  (0, 405)\t0.002625567388985271\n",
      "  (0, 152)\t0.002625567388985271\n",
      "  (0, 109)\t0.002625567388985271\n",
      "  (0, 301)\t0.002625567388985271\n",
      "  (0, 669)\t0.002625567388985271\n",
      "  (0, 773)\t0.002625567388985271\n",
      "  (0, 815)\t0.002625567388985271\n",
      "  (0, 24)\t0.002625567388985271\n",
      "  (0, 387)\t0.002625567388985271\n",
      "  (0, 716)\t0.002625567388985271\n",
      "  (0, 460)\t0.002625567388985271\n",
      "  (0, 753)\t0.002625567388985271\n",
      "  (0, 310)\t0.002625567388985271\n",
      "  (0, 374)\t0.002625567388985271\n",
      "  (0, 576)\t0.002625567388985271\n",
      "  (0, 89)\t0.002625567388985271\n",
      "  (0, 8)\t0.002625567388985271\n",
      "  (0, 564)\t0.002625567388985271\n",
      "  (0, 92)\t0.002625567388985271\n",
      "  (0, 560)\t0.002625567388985271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ih8l1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define paths\n",
    "project_folder = './'  # Modify this if your project folder is located elsewhere\n",
    "idioms_file_path = os.path.join(project_folder, 'idioms.txt')\n",
    "files_text_folder = os.path.join(project_folder, 'Files_Text')\n",
    "\n",
    "# Function to read file\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Read and vectorize idioms.txt\n",
    "idioms_text = read_file(idioms_file_path)\n",
    "\n",
    "if not idioms_text:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "# Tokenization and vectorization\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "idioms_vector = vectorizer.fit_transform([idioms_text])\n",
    "\n",
    "print(\"Idioms Vectorized:\")\n",
    "print(idioms_vector)\n",
    "\n",
    "# Tokenize all files in Files_Text folder\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Read and tokenize files in Files_Text\n",
    "files_text_files = glob.glob(os.path.join(files_text_folder, '*.txt'))\n",
    "tokenized_texts = {}\n",
    "for file_path in files_text_files:\n",
    "    file_content = read_file(file_path)\n",
    "    if file_content:\n",
    "        tokenized_texts[file_path] = tokenize_text(file_content)\n",
    "        print(f\"Tokenized {file_path} with {len(tokenized_texts[file_path])} tokens.\")\n",
    "    else:\n",
    "        print(f\"Skipped empty file: {file_path}\")\n",
    "\n",
    "# Output tokenized results (stored in the dictionary tokenized_texts)\n",
    "for file_path, tokens in tokenized_texts.items():\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    print(f\"Tokens: {tokens[:10]}...\")  # Printing only the first 10 tokens for brevity\n",
    "\n",
    "# If you need the complete token list, you can store or process `tokenized_texts` further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ih8l1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 12125 characters from ./idioms.txt\n",
      "Extracted 717 idioms.\n",
      "Found 83 files in ./Text_Files.\n",
      "Read 83528 characters from ./Text_Files\\fomcminutes20140129.txt\n",
      "Read 88557 characters from ./Text_Files\\fomcminutes20140319.txt\n",
      "Read 47125 characters from ./Text_Files\\fomcminutes20140430.txt\n",
      "Read 92817 characters from ./Text_Files\\fomcminutes20140618.txt\n",
      "Read 55607 characters from ./Text_Files\\fomcminutes20140730.txt\n",
      "Read 99528 characters from ./Text_Files\\fomcminutes20140917.txt\n",
      "Read 58231 characters from ./Text_Files\\fomcminutes20141029.txt\n",
      "Read 89187 characters from ./Text_Files\\fomcminutes20141217.txt\n",
      "Read 101017 characters from ./Text_Files\\fomcminutes20150128.txt\n",
      "Read 93324 characters from ./Text_Files\\fomcminutes20150318.txt\n",
      "Read 53467 characters from ./Text_Files\\fomcminutes20150429.txt\n",
      "Read 91098 characters from ./Text_Files\\fomcminutes20150617.txt\n",
      "Read 59250 characters from ./Text_Files\\fomcminutes20150729.txt\n",
      "Read 89178 characters from ./Text_Files\\fomcminutes20150917.txt\n",
      "Read 57113 characters from ./Text_Files\\fomcminutes20151028.txt\n",
      "Read 86885 characters from ./Text_Files\\fomcminutes20151216.txt\n",
      "Read 95915 characters from ./Text_Files\\fomcminutes20160127.txt\n",
      "Read 87020 characters from ./Text_Files\\fomcminutes20160316.txt\n",
      "Read 64880 characters from ./Text_Files\\fomcminutes20160427.txt\n",
      "Read 87946 characters from ./Text_Files\\fomcminutes20160615.txt\n",
      "Read 72160 characters from ./Text_Files\\fomcminutes20160727.txt\n",
      "Read 108725 characters from ./Text_Files\\fomcminutes20160921.txt\n",
      "Read 64506 characters from ./Text_Files\\fomcminutes20161102.txt\n",
      "Read 90961 characters from ./Text_Files\\fomcminutes20161214.txt\n",
      "Read 88254 characters from ./Text_Files\\fomcminutes20170201.txt\n",
      "Read 109855 characters from ./Text_Files\\fomcminutes20170315.txt\n",
      "Read 60117 characters from ./Text_Files\\fomcminutes20170503.txt\n",
      "Read 109472 characters from ./Text_Files\\fomcminutes20170614.txt\n",
      "Read 52776 characters from ./Text_Files\\fomcminutes20170726.txt\n",
      "Read 106648 characters from ./Text_Files\\fomcminutes20170920.txt\n",
      "Read 54794 characters from ./Text_Files\\fomcminutes20171101.txt\n",
      "Read 97049 characters from ./Text_Files\\fomcminutes20171213.txt\n",
      "Read 88052 characters from ./Text_Files\\fomcminutes20180131.txt\n",
      "Read 96041 characters from ./Text_Files\\fomcminutes20180321.txt\n",
      "Read 56082 characters from ./Text_Files\\fomcminutes20180502.txt\n",
      "Read 92976 characters from ./Text_Files\\fomcminutes20180613.txt\n",
      "Read 59728 characters from ./Text_Files\\fomcminutes20180801.txt\n",
      "Read 93205 characters from ./Text_Files\\fomcminutes20180926.txt\n",
      "Read 58935 characters from ./Text_Files\\fomcminutes20181108.txt\n",
      "Read 104639 characters from ./Text_Files\\fomcminutes20181219.txt\n",
      "Read 100398 characters from ./Text_Files\\fomcminutes20190130.txt\n",
      "Read 109152 characters from ./Text_Files\\fomcminutes20190320.txt\n",
      "Read 65316 characters from ./Text_Files\\fomcminutes20190501.txt\n",
      "Read 102541 characters from ./Text_Files\\fomcminutes20190619.txt\n",
      "Read 75160 characters from ./Text_Files\\fomcminutes20190731.txt\n",
      "Read 119495 characters from ./Text_Files\\fomcminutes20190918.txt\n",
      "Read 88737 characters from ./Text_Files\\fomcminutes20191030.txt\n",
      "Read 103252 characters from ./Text_Files\\fomcminutes20191211.txt\n",
      "Read 93507 characters from ./Text_Files\\fomcminutes20200129.txt\n",
      "Read 69899 characters from ./Text_Files\\fomcminutes20200315.txt\n",
      "Read 66866 characters from ./Text_Files\\fomcminutes20200429.txt\n",
      "Read 103677 characters from ./Text_Files\\fomcminutes20200610.txt\n",
      "Read 64058 characters from ./Text_Files\\fomcminutes20200729.txt\n",
      "Read 102556 characters from ./Text_Files\\fomcminutes20200916.txt\n",
      "Read 67300 characters from ./Text_Files\\fomcminutes20201105.txt\n",
      "Read 59323 characters from ./Text_Files\\fomcminutes20201216.txt\n",
      "Read 90088 characters from ./Text_Files\\fomcminutes20210127.txt\n",
      "Read 61924 characters from ./Text_Files\\fomcminutes20210317.txt\n",
      "Read 62112 characters from ./Text_Files\\fomcminutes20210428.txt\n",
      "Read 67859 characters from ./Text_Files\\fomcminutes20210616.txt\n",
      "Read 74672 characters from ./Text_Files\\fomcminutes20210728.txt\n",
      "Read 61689 characters from ./Text_Files\\fomcminutes20210922.txt\n",
      "Read 58678 characters from ./Text_Files\\fomcminutes20211103.txt\n",
      "Read 66221 characters from ./Text_Files\\fomcminutes20211215.txt\n",
      "Read 94953 characters from ./Text_Files\\fomcminutes20220126.txt\n",
      "Read 61167 characters from ./Text_Files\\fomcminutes20220316.txt\n",
      "Read 55731 characters from ./Text_Files\\fomcminutes20220504.txt\n",
      "Read 55946 characters from ./Text_Files\\fomcminutes20220615.txt\n",
      "Read 60478 characters from ./Text_Files\\fomcminutes20220727.txt\n",
      "Read 55101 characters from ./Text_Files\\fomcminutes20220921.txt\n",
      "Read 57639 characters from ./Text_Files\\fomcminutes20221102.txt\n",
      "Read 55227 characters from ./Text_Files\\fomcminutes20221214.txt\n",
      "Read 62047 characters from ./Text_Files\\fomcminutes20230201.txt\n",
      "Read 53277 characters from ./Text_Files\\fomcminutes20230322.txt\n",
      "Read 58507 characters from ./Text_Files\\fomcminutes20230503.txt\n",
      "Read 50105 characters from ./Text_Files\\fomcminutes20230614.txt\n",
      "Read 49081 characters from ./Text_Files\\fomcminutes20230726.txt\n",
      "Read 46423 characters from ./Text_Files\\fomcminutes20230920.txt\n",
      "Read 49366 characters from ./Text_Files\\fomcminutes20231101.txt\n",
      "Read 48902 characters from ./Text_Files\\fomcminutes20231213.txt\n",
      "Read 53532 characters from ./Text_Files\\fomcminutes20240131.txt\n",
      "Read 51592 characters from ./Text_Files\\fomcminutes20240320.txt\n",
      "Read 51126 characters from ./Text_Files\\fomcminutes20240501.txt\n",
      "\n",
      "Exact Matches:\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140129.txt\n",
      "  Matching Idioms: ['calendar year', 'all set']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140319.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140430.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140618.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140730.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20140917.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20141029.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20141217.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150128.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150318.txt\n",
      "  Matching Idioms: ['in the pipeline', 'calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150429.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150617.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150729.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20150917.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20151028.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20151216.txt\n",
      "  Matching Idioms: ['close call', 'calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160127.txt\n",
      "  Matching Idioms: ['calendar year', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160316.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160427.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160615.txt\n",
      "  Matching Idioms: ['calendar year', 'in the wake of', 'bank on']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160727.txt\n",
      "  Matching Idioms: ['under pressure', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20160921.txt\n",
      "  Matching Idioms: ['close call', 'calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20161102.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20161214.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170201.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170315.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170503.txt\n",
      "  Matching Idioms: ['in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170614.txt\n",
      "  Matching Idioms: ['calendar year', 'along the lines']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170726.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20170920.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20171101.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20171213.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180131.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180321.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180502.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180613.txt\n",
      "  Matching Idioms: ['under pressure', 'calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180801.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20180926.txt\n",
      "  Matching Idioms: ['under pressure', 'calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20181108.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20181219.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190130.txt\n",
      "  Matching Idioms: ['calendar year', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190320.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190501.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190619.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190731.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20190918.txt\n",
      "  Matching Idioms: ['calendar year', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20191030.txt\n",
      "  Matching Idioms: ['close call']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20191211.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200129.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200315.txt\n",
      "  Matching Idioms: ['in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200429.txt\n",
      "  Matching Idioms: ['across the board', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200610.txt\n",
      "  Matching Idioms: ['calendar year', 'in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200729.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20200916.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20201105.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20201216.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210127.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210317.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210428.txt\n",
      "  Matching Idioms: ['brought forward']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210616.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210728.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20210922.txt\n",
      "  Matching Idioms: ['in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20211103.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20211215.txt\n",
      "  Matching Idioms: ['on the run']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220126.txt\n",
      "  Matching Idioms: ['calendar year']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220316.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220504.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220615.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220727.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220921.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20221102.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20221214.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230201.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230322.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230503.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230614.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230726.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230920.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20231101.txt\n",
      "  Matching Idioms: ['under pressure']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20231213.txt\n",
      "  No exact idiom matches.\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240131.txt\n",
      "  Matching Idioms: ['under pressure']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240320.txt\n",
      "  Matching Idioms: ['in the wake of']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240501.txt\n",
      "  No exact idiom matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Results:\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20220921.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20221214.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230201.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230322.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230503.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230614.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230726.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20230920.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20231101.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20231213.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240131.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240320.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: ./Text_Files\\fomcminutes20240501.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define paths\n",
    "project_folder = './'  # Modify this if your project folder is located elsewhere\n",
    "idioms_file_path = os.path.join(project_folder, 'idioms.txt')\n",
    "text_files_folder = os.path.join(project_folder, 'Text_Files')\n",
    "\n",
    "# Function to read file\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "            print(f\"Read {len(content)} characters from {file_path}\")\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Read idioms.txt and extract idioms\n",
    "idioms_text = read_file(idioms_file_path)\n",
    "idioms = [line.strip() for line in idioms_text.splitlines() if line.strip()]\n",
    "\n",
    "if not idioms:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "print(f\"Extracted {len(idioms)} idioms.\")\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())  # Lowercase for case-insensitive matching\n",
    "\n",
    "# Read and tokenize files in Text_Files\n",
    "text_files_paths = glob.glob(os.path.join(text_files_folder, '*.txt'))\n",
    "print(f\"Found {len(text_files_paths)} files in {text_files_folder}.\")\n",
    "file_contents = [read_file(file_path) for file_path in text_files_paths]\n",
    "\n",
    "# Filter out empty texts\n",
    "file_contents = [text for text in file_contents if text]\n",
    "text_files_paths = [file_path for file_path, text in zip(text_files_paths, file_contents) if text]\n",
    "\n",
    "if not text_files_paths:\n",
    "    raise ValueError(\"No valid content in Text_Files.\")\n",
    "\n",
    "# Check for exact matches and calculate similarities\n",
    "results = {}\n",
    "for file_path, content in zip(text_files_paths, file_contents):\n",
    "    tokens = tokenize_text(content)\n",
    "    matching_idioms = [idiom for idiom in idioms if idiom.lower() in content.lower()]\n",
    "    if matching_idioms:\n",
    "        results[file_path] = matching_idioms\n",
    "    else:\n",
    "        results[file_path] = \"No exact idiom matches.\"\n",
    "\n",
    "# Output the results of exact matches\n",
    "print(\"\\nExact Matches:\")\n",
    "for file_path, matches in results.items():\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    if isinstance(matches, list):\n",
    "        print(f\"  Matching Idioms: {matches}\")\n",
    "    else:\n",
    "        print(f\"  {matches}\")\n",
    "\n",
    "# Vectorization for similarity check (optional)\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', lowercase=True, ngram_range=(1, 2))\n",
    "all_texts = idioms + file_contents\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Separate idioms vectors and text file vectors\n",
    "idioms_vectors = tfidf_matrix[:len(idioms)]\n",
    "text_files_vectors = tfidf_matrix[len(idioms):]\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity(text_files_vectors, idioms_vectors)\n",
    "\n",
    "# Output the similarity results\n",
    "similarity_threshold = 0.1  # Set a threshold for remote similarity\n",
    "print(\"\\nSimilarity Results:\")\n",
    "for idx, file_path in enumerate(text_files_paths):\n",
    "    similar_idioms = [idioms[j] for j in range(len(idioms)) if similarities[idx][j] >= similarity_threshold]\n",
    "    if similar_idioms:\n",
    "        print(f\"\\nFile: {file_path}\")\n",
    "        print(f\"  Similar Idioms (threshold {similarity_threshold}): {similar_idioms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphor Ratio in 'fomcminutes20140129.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20140319.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20140430.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20140618.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20140730.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20140917.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20141029.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20141217.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150128.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150318.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150429.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150617.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150729.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20150917.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20151028.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20151216.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160127.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160316.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160427.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160615.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160727.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20160921.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20161102.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20161214.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170201.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170315.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170503.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170614.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170726.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20170920.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20171101.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20171213.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180131.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180321.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180502.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180613.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180801.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20180926.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20181108.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20181219.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190130.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190320.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190501.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190619.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190731.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20190918.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20191030.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20191211.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200129.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200315.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200429.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200610.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200729.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20200916.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20201105.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20201216.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210127.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210317.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210428.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210616.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210728.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20210922.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20211103.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20211215.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220126.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220316.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220504.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220615.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220727.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20220921.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20221102.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20221214.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230201.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230322.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230503.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230614.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230726.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20230920.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20231101.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20231213.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20240131.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20240320.txt': 0.0\n",
      "Metaphor Ratio in 'fomcminutes20240501.txt': 0.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to analyze metaphors in a given text\n",
    "def analyze_metaphors(text):\n",
    "  # Process the text with spaCy\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Initialize a counter for metaphors\n",
    "  metaphor_count = 0\n",
    "  \n",
    "  # Iterate through each token in the document\n",
    "  for token in doc:\n",
    "    # Check if the token is a metaphor\n",
    "    if token.dep_ == \"metaphor\":\n",
    "      metaphor_count += 1\n",
    "  \n",
    "  # Calculate the ratio of metaphors to total words\n",
    "  total_words = len(doc)\n",
    "  metaphor_ratio = metaphor_count / total_words if total_words > 0 else 0\n",
    "  \n",
    "  return metaphor_ratio\n",
    "\n",
    "# Function to read a text file and return its content as a string\n",
    "def read_text_file(filepath):\n",
    "  with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "  return text\n",
    "\n",
    "# Get the path to the Text_Files folder\n",
    "text_folder_path = \"Text_Files\"\n",
    "\n",
    "# Get all filenames within the folder\n",
    "filenames = os.listdir(text_folder_path)\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "metaphor_ratios = []\n",
    "\n",
    "# Loop through each filename\n",
    "for filename in filenames:\n",
    "  # Construct the complete file path\n",
    "  file_path = os.path.join(text_folder_path, filename)\n",
    "  \n",
    "  # Read the content of the file\n",
    "  text = read_text_file(file_path)\n",
    "  \n",
    "  # Analyze metaphors in the text\n",
    "  metaphor_ratio = analyze_metaphors(text)\n",
    "  \n",
    "  # Append the metaphor ratio to the results list\n",
    "  metaphor_ratios.append(metaphor_ratio)\n",
    "\n",
    "# Print the results for each file\n",
    "for i, filename in enumerate(filenames):\n",
    "  print(f\"Metaphor Ratio in '{filename}':\", metaphor_ratios[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE THE IMAGE AND SIMPLIFY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

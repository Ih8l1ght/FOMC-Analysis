{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idioms and Metaphores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries and Download NLTK Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "Spacy package works only below Python 3.12 (current is 3.13/3.14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     ConfigSchema,\n\u001b[0;32m     46\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     validate_init_settings,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTokenPatternString\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mREGEX\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTokenPatternString\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIN\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_update_forward_refs__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:807\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m     \u001b[43mupdate_model_forward_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__config__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mNameError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         \u001b[43mupdate_field_forward_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocalns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         \u001b[43mupdate_field_forward_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocalns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_forwardref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ih8l1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Defining Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = \"Datum\"\n",
    "idioms_file_path = \"Datum/idioms.txt\"\n",
    "files_text_folder = \"Text_Files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Idioms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short List of Idioms Vectorized:\n",
      "  (0, 406)\t0.007876702166955815\n",
      "  (0, 137)\t0.005251134777970542\n",
      "  (0, 678)\t0.002625567388985271\n",
      "  (0, 654)\t0.007876702166955815\n"
     ]
    }
   ],
   "source": [
    "idioms_text = read_file(idioms_file_path)\n",
    "if not idioms_text:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "idioms_vector = vectorizer.fit_transform([idioms_text])\n",
    "\n",
    "N = 20\n",
    "\n",
    "print(\"Short List of Idioms Vectorized:\")\n",
    "print(str(idioms_vector)[:127])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text Files in `Files_Text` Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text_Files\\fomcminutes20140129.txt with 13848 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20140319.txt with 14681 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20140430.txt with 7777 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20140618.txt with 15747 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20140730.txt with 9149 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20140917.txt with 16608 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20141029.txt with 9509 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20141217.txt with 14761 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150128.txt with 16785 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150318.txt with 15569 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150429.txt with 8768 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150617.txt with 15189 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150729.txt with 9645 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20150917.txt with 14715 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20151028.txt with 9312 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20151216.txt with 14272 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160127.txt with 15812 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160316.txt with 14322 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160427.txt with 10638 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160615.txt with 14886 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160727.txt with 11943 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20160921.txt with 17884 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20161102.txt with 10552 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20161214.txt with 14966 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170201.txt with 14523 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170315.txt with 18063 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170503.txt with 9909 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170614.txt with 18055 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170726.txt with 8707 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20170920.txt with 17654 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20171101.txt with 8854 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20171213.txt with 16008 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180131.txt with 14245 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180321.txt with 15875 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180502.txt with 9182 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180613.txt with 15431 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180801.txt with 9668 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20180926.txt with 15442 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20181108.txt with 9650 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20181219.txt with 17181 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190130.txt with 16411 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190320.txt with 18119 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190501.txt with 10791 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190619.txt with 16886 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190731.txt with 12378 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20190918.txt with 19547 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20191030.txt with 14550 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20191211.txt with 16766 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200129.txt with 15392 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200315.txt with 11502 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200429.txt with 10938 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200610.txt with 16815 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200729.txt with 10536 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20200916.txt with 16514 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20201105.txt with 10884 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20201216.txt with 9638 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210127.txt with 14799 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210317.txt with 10105 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210428.txt with 10100 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210616.txt with 11178 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210728.txt with 12349 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20210922.txt with 10192 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20211103.txt with 9607 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20211215.txt with 10845 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220126.txt with 15743 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220316.txt with 10056 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220504.txt with 9215 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220615.txt with 9331 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220727.txt with 9990 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20220921.txt with 9036 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20221102.txt with 9428 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20221214.txt with 9078 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230201.txt with 10165 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230322.txt with 8786 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230503.txt with 9672 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230614.txt with 8275 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230726.txt with 8151 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20230920.txt with 7708 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20231101.txt with 8097 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20231213.txt with 8020 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20240131.txt with 8821 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20240320.txt with 8484 tokens.\n",
      "Tokenized Text_Files\\fomcminutes20240501.txt with 8420 tokens.\n",
      "\n",
      "File: Text_Files\\fomcminutes20140129.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '28–29', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20140319.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '18–19', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20140430.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '29–30', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20140618.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '17', '–18']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20140730.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '29–30', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20140917.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '16', '–17']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20141029.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'October', '28–29', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20141217.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '16', '–17']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150128.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '27', '–28']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150318.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '17–18', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150429.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '28–29', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150617.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '16', '–17']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150729.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '28', '–29']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20150917.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '16', '–17']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20151028.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'October', '27–28', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20151216.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '15', '–16']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160127.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '26', '–27']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160316.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '15', '–16']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160427.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '26–27', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160615.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '14–15', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160727.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '26', '–27']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20160921.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '20', '–21']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20161102.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'November', '1–2', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20161214.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '13', '–14']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170201.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '31', '–February']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170315.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '14', '–15']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170503.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'May', '2', '–3']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170614.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '13–', '14']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170726.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '25', '–26']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20170920.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '19', '–20']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20171101.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'October', '31', '–November']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20171213.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '12–13', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180131.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '30', '–31']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180321.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '20', '–21']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180502.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'May', '1–2', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180613.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '12–', '13']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180801.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '31', '–August']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20180926.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '25', '–26']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20181108.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'November', '7', '–8']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20181219.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '18', '–19']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190130.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '29', '–30']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190320.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '19–20', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190501.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '30–May', '1']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190619.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '18–', '19']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190731.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '30–31', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20190918.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '17–18', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20191030.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'October', '29–30', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20191211.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '10–1', '1']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200129.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '28', '–29']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200315.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '15', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200429.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '28–29', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200610.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '9–10', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200729.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '28–29', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20200916.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '15', '–16']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20201105.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'November', '4–5', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20201216.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '15–', '16']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210127.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '26–27', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210317.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '16–17', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210428.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '27–28', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210616.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '15–16', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210728.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '27–28', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20210922.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '21–22', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20211103.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'November', '2–3', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20211215.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '14–15', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220126.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '25–2', '6']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220316.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '15–', '16']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220504.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'May', '3–4', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220615.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '14–15', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220727.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '26', '–27']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20220921.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '20–21', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20221102.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'Novem', 'ber', '1–']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20221214.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '13–14', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230201.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '31', '–February']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230322.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '21–', '22']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230503.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'May', '2–3', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230614.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'June', '13–14', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230726.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'July', '25–2', '6']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20230920.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'September', '19–20', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20231101.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'October', '31', '–November']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20231213.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'December', '12–13', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20240131.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'January', '30–31', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20240320.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'March', '19–20', ',']...\n",
      "\n",
      "File: Text_Files\\fomcminutes20240501.txt\n",
      "Tokens: ['Minutes', 'of', 'the', 'Federal', 'Open', 'Market', 'Committee', 'April', '30', '–May']...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "files_text_files = glob.glob(os.path.join(files_text_folder, '*.txt'))\n",
    "tokenized_texts = {}\n",
    "for file_path in files_text_files:\n",
    "    file_content = read_file(file_path)\n",
    "    if file_content:\n",
    "        tokenized_texts[file_path] = tokenize_text(file_content)\n",
    "        print(f\"Tokenized {file_path} with {len(tokenized_texts[file_path])} tokens.\")\n",
    "    else:\n",
    "        print(f\"Skipped empty file: {file_path}\")\n",
    "\n",
    "# Output tokenized results\n",
    "for file_path, tokens in tokenized_texts.items():\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    print(f\"Tokens: {tokens[:10]}...\")  # Printing only the first 10 tokens for braveness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Idioms and Extracting from Text Files in `Text_Files` Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 717 idioms.\n",
      "Found 83 files in Text_Files.\n"
     ]
    }
   ],
   "source": [
    "# Define new folder path\n",
    "text_files_folder = 'Text_Files'\n",
    "\n",
    "idioms_text = read_file(idioms_file_path)\n",
    "idioms = [line.strip() for line in idioms_text.splitlines() if line.strip()]\n",
    "if not idioms:\n",
    "    raise ValueError(f\"No valid content in {idioms_file_path}.\")\n",
    "\n",
    "print(f\"Extracted {len(idioms)} idioms.\")\n",
    "\n",
    "# Tokenize Text Files\n",
    "text_files_paths = glob.glob(os.path.join(text_files_folder, '*.txt'))\n",
    "print(f\"Found {len(text_files_paths)} files in {text_files_folder}.\")\n",
    "file_contents = [read_file(file_path) for file_path in text_files_paths]\n",
    "\n",
    "# Filter out empty texts\n",
    "file_contents = [text for text in file_contents if text]\n",
    "text_files_paths = [file_path for file_path, text in zip(text_files_paths, file_contents) if text]\n",
    "\n",
    "if not text_files_paths:\n",
    "    raise ValueError(\"No valid content in Text_Files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Exact Matches and Calculate Similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results = {}\n",
    "for file_path, content in zip(text_files_paths, file_contents):\n",
    "    tokens = tokenize_text(content)\n",
    "    matching_idioms = [idiom for idiom in idioms if idiom.lower() in content.lower()]\n",
    "    if matching_idioms:\n",
    "        results[file_path] = matching_idioms\n",
    "    else:\n",
    "        results[file_path] = \"No exact idiom matches.\"\n",
    "'''\n",
    "# Output exact matches\n",
    "print(\"\\nExact Matches:\")\n",
    "for file_path, matches in results.items():\n",
    "    #print(f\"\\nFile: {file_path}\")\n",
    "    if isinstance(matches, list):\n",
    "        print(f\"  Matching Idioms: {matches} in {file_path}\")\n",
    "    else:\n",
    "        print(f\"  {matches}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and Similarity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Results:\n",
      "\n",
      "File: Text_Files\\fomcminutes20220921.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20221214.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230201.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230322.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230503.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230614.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230726.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20230920.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20231101.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20231213.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20240131.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20240320.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n",
      "\n",
      "File: Text_Files\\fomcminutes20240501.txt\n",
      "  Similar Idioms (threshold 0.1): ['across the board', 'get on board']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', lowercase=True, ngram_range=(1, 2))\n",
    "all_texts = idioms + file_contents\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Separate idioms vectors and text file vectors\n",
    "idioms_vectors = tfidf_matrix[:len(idioms)]\n",
    "text_files_vectors = tfidf_matrix[len(idioms):]\n",
    "\n",
    "similarities = cosine_similarity(text_files_vectors, idioms_vectors)\n",
    "\n",
    "similarity_threshold = 0.1    # After decreasing it starts associating with completely different words\n",
    "print(\"\\nSimilarity Results:\")\n",
    "for idx, file_path in enumerate(text_files_paths):\n",
    "    similar_idioms = [idioms[j] for j in range(len(idioms)) if similarities[idx][j] >= similarity_threshold]\n",
    "    if similar_idioms:\n",
    "        print(f\"\\nFile: {file_path}\")\n",
    "        print(f\"  Similar Idioms (threshold {similarity_threshold}): {similar_idioms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# From Mixed Nuts to Metaphors: Decoding the Nuts and Bolts of Figurative Speech with SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the spaCy English model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to analyze metaphors in a given text\n",
    "def analyze_metaphors(text):\n",
    "    doc = nlp(text)\n",
    "    metaphor_count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"metaphor\":\n",
    "            metaphor_count += 1\n",
    "    return metaphor_count\n",
    "\n",
    "# Function to read a text file and return its content as a striiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiing\n",
    "def read_text_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Get the path to the Text_Files folder\n",
    "text_folder_path = \"Text_Files\"\n",
    "\n",
    "# Get all filenames within the folder\n",
    "filenames = os.listdir(text_folder_path)\n",
    "\n",
    "list_as_empty_as_my_wallet = []\n",
    "\n",
    "# Loopy loop\n",
    "for filename in filenames:\n",
    "    metaphor_n = analyze_metaphors(text)\n",
    "    list_as_empty_as_my_wallet.append(metaphor_n)\n",
    "print(list_as_empty_as_my_wallet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
